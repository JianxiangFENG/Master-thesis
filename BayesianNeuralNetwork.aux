\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Bayesian neural network}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{9}{section.3.1}\protected@file@percent }
\newlabel{2.1}{{3.1}{9}{Introduction}{equation.3.1.1}{}}
\newlabel{2.2}{{3.2}{9}{Introduction}{equation.3.1.2}{}}
\newlabel{2.3}{{3.3}{10}{Introduction}{equation.3.1.3}{}}
\newlabel{2.4}{{3.4}{10}{Introduction}{equation.3.1.4}{}}
\newlabel{2.5}{{3.5}{10}{Introduction}{equation.3.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Difference between parameter estimation of deterministic neural network and Bayesian neural network.}}{11}{figure.3.1}\protected@file@percent }
\newlabel{fig:dnn_bnn}{{3.1}{11}{Difference between parameter estimation of deterministic neural network and Bayesian neural network}{figure.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Variational inference}{11}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction}{11}{subsection.3.2.1}\protected@file@percent }
\citation{graves2011practical}
\newlabel{2.6}{{3.6}{12}{Introduction}{equation.3.2.6}{}}
\newlabel{2.7}{{3.7}{12}{Introduction}{equation.3.2.7}{}}
\newlabel{2.8}{{3.8}{12}{Introduction}{equation.3.2.8}{}}
\newlabel{2.9}{{3.9}{12}{Introduction}{equation.3.2.9}{}}
\citation{srivastava2014dropout}
\citation{srivastava2014dropout}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Dropout variational inference}{13}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dropout}{13}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces How dropout works\cite  {srivastava2014dropout}.}}{13}{figure.3.2}\protected@file@percent }
\newlabel{fig:dropout}{{3.2}{13}{How dropout works\cite {srivastava2014dropout}}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Bayesian interpretation of dropout}{14}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A two layer neural network example of dropout inference, a Bernoulli random variable is imposed on each unit of input layer and hidden layer.}}{14}{figure.3.3}\protected@file@percent }
\newlabel{fig:dropout_inference}{{3.3}{14}{A two layer neural network example of dropout inference, a Bernoulli random variable is imposed on each unit of input layer and hidden layer}{figure.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Approximate distribution}{15}{section*.5}\protected@file@percent }
\newlabel{dropout_form}{{3.10}{15}{Approximate distribution}{equation.3.2.10}{}}
\newlabel{appro_dist_form}{{3.11}{16}{Approximate distribution}{equation.3.2.11}{}}
\newlabel{appro_cond_dist_form}{{3.12}{16}{Approximate distribution}{equation.3.2.12}{}}
\newlabel{appr_expectation}{{3.13}{16}{Approximate distribution}{equation.3.2.13}{}}
\newlabel{appr_covariance}{{3.14}{16}{Approximate distribution}{equation.3.2.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Training objective}{17}{section*.6}\protected@file@percent }
\newlabel{dropout_loss}{{3.15}{17}{Training objective}{equation.3.2.15}{}}
\citation{kingma2013auto}
\citation{gal2016uncertainty}
\citation{williams1992simple}
\citation{kingma2013auto}
\citation{kingma2013auto}
\newlabel{dropout_grad}{{3.16}{18}{Training objective}{equation.3.2.16}{}}
\newlabel{elbo_grad}{{3.17}{18}{Training objective}{equation.3.2.17}{}}
\@writefile{toc}{\contentsline {subparagraph}{Re-parameterization trick:}{18}{section*.7}\protected@file@percent }
\newlabel{repa}{{3.18}{18}{Re-parameterization trick:}{equation.3.2.18}{}}
\newlabel{repa1}{{3.19}{19}{Re-parameterization trick:}{equation.3.2.19}{}}
\@writefile{toc}{\contentsline {subparagraph}{KL condition:}{19}{section*.8}\protected@file@percent }
\newlabel{appro_dist_guassian}{{3.20}{20}{KL condition:}{equation.3.2.20}{}}
\newlabel{kl_condition}{{3.21}{20}{KL condition:}{equation.3.2.21}{}}
\newlabel{KL_grad}{{3.22}{20}{KL condition:}{equation.3.2.22}{}}
\citation{srivastava2014dropout}
\citation{gal2017concrete}
\citation{kingma2013auto}
\citation{maddison2016concrete}
\@writefile{toc}{\contentsline {paragraph}{Marginalization in testing}{21}{section*.9}\protected@file@percent }
\newlabel{marginalization_test}{{3.23}{21}{Marginalization in testing}{equation.3.2.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Concrete dropout and Multi-Drop}{21}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Concrete dropout}{21}{section*.10}\protected@file@percent }
\citation{jang2016categorical}
\citation{maddison2016concrete}
\citation{maddison2014sampling}
\citation{maddison2016concrete}
\citation{maddison2016concrete}
\@writefile{toc}{\contentsline {subparagraph}{Re-parameterization of Bernoulli distribution}{22}{section*.11}\protected@file@percent }
\newlabel{gumbel_softmax}{{3.24}{22}{Re-parameterization of Bernoulli distribution}{equation.3.2.24}{}}
\citation{maddison2016concrete}
\citation{maddison2016concrete}
\newlabel{gumbel_max}{{3.2.3}{23}{Re-parameterization of Bernoulli distribution}{section*.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$ \cite  {maddison2016concrete}.}}{23}{figure.3.4}\protected@file@percent }
\newlabel{fig:gumbel_max}{{3.4}{23}{Example of Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$ \cite {maddison2016concrete}}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Example of continuous approximation to Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$\cite  {maddison2016concrete}.}}{23}{figure.3.5}\protected@file@percent }
\newlabel{fig:gumbel_softmax}{{3.5}{23}{Example of continuous approximation to Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$\cite {maddison2016concrete}}{figure.3.5}{}}
\newlabel{bern_repa}{{3.25}{24}{Re-parameterization of Bernoulli distribution}{equation.3.2.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces One sample and average value of 100 samples drawn from continuous approximation of Bernoulli distribution with parameter $p = [0.1, 0.2, ..., 1.0]$ and temperature $ \lambda =0.1$.}}{24}{figure.3.6}\protected@file@percent }
\newlabel{fig:bern_repa}{{3.6}{24}{One sample and average value of 100 samples drawn from continuous approximation of Bernoulli distribution with parameter $p = [0.1, 0.2, ..., 1.0]$ and temperature $ \lambda =0.1$}{figure.3.6}{}}
\@writefile{toc}{\contentsline {subparagraph}{Dropout regularization}{25}{section*.12}\protected@file@percent }
\newlabel{KL_grad_concrete}{{3.26}{25}{Dropout regularization}{equation.3.2.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Multi-Drop}{25}{section.3.3}\protected@file@percent }
\newlabel{KL_grad_multi}{{3.27}{25}{Multi-Drop}{equation.3.3.27}{}}
\newlabel{fig:cdp_dropout1}{{3.3}{26}{Multi-Drop}{equation.3.3.27}{}}
\newlabel{fig:cdp_dropout2}{{3.3}{26}{Multi-Drop}{equation.3.3.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Changes along epochs of keep probability in training network with concrete dropout layer.}}{26}{figure.3.7}\protected@file@percent }
\citation{he2016deep}
\citation{srivastava2014dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Different dropout rates for different hidden units in multi-drop.}}{27}{figure.3.8}\protected@file@percent }
\newlabel{fig:multi-drop}{{3.8}{27}{Different dropout rates for different hidden units in multi-drop}{figure.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Modified network architecture}{27}{subsection.3.3.1}\protected@file@percent }
\citation{bishop2006pattern}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Modified network architecture.}}{28}{figure.3.9}\protected@file@percent }
\newlabel{fig:modified_net}{{3.9}{28}{Modified network architecture}{figure.3.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Laplace approximation}{28}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Introduction}{28}{subsection.3.4.1}\protected@file@percent }
\newlabel{point estimate}{{3.28}{29}{Introduction}{equation.3.4.28}{}}
\newlabel{taylor expansion}{{3.29}{29}{Introduction}{equation.3.4.29}{}}
\newlabel{gaussian form}{{3.30}{29}{Introduction}{equation.3.4.30}{}}
\newlabel{expected hessian}{{3.31}{30}{Introduction}{equation.3.4.31}{}}
\@writefile{toc}{\contentsline {paragraph}{Fisher information matrix}{30}{section*.13}\protected@file@percent }
\newlabel{Fisher}{{3.32}{30}{Fisher information matrix}{equation.3.4.32}{}}
\citation{ritter2018scalable}
\citation{martens2015optimizing}
\newlabel{fisher hessian}{{3.33}{31}{Fisher information matrix}{equation.3.4.33}{}}
\newlabel{fisher hessian_with_prior}{{3.34}{31}{Fisher information matrix}{equation.3.4.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Scalable Laplace approximation for neural network}{31}{subsection.3.4.2}\protected@file@percent }
\citation{gupta1999matrix}
\newlabel{fisher_appr}{{3.37}{32}{Scalable Laplace approximation for neural network}{equation.3.4.37}{}}
\newlabel{mvg}{{3.38}{33}{Scalable Laplace approximation for neural network}{equation.3.4.38}{}}
\newlabel{mvg_prior}{{3.39}{33}{Scalable Laplace approximation for neural network}{equation.3.4.39}{}}
\@setckpt{BayesianNeuralNetwork}{
\setcounter{page}{34}
\setcounter{equation}{39}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{0}
\setcounter{StandardModuleDepth}{0}
\setcounter{AM@survey}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{19}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{section@level}{2}
}
