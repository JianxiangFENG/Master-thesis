\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{gal2016dropout}
\citation{ritter2018scalable}
\citation{gal2017concrete}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian neural network}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{9}{section.2.1}\protected@file@percent }
\newlabel{2.1}{{2.1}{9}{Introduction}{equation.2.1.1}{}}
\newlabel{2.2}{{2.2}{9}{Introduction}{equation.2.1.2}{}}
\newlabel{2.3}{{2.3}{9}{Introduction}{equation.2.1.3}{}}
\newlabel{2.4}{{2.4}{9}{Introduction}{equation.2.1.4}{}}
\newlabel{2.5}{{2.5}{10}{Introduction}{equation.2.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Difference between parameter estimation of deterministic neural network and Bayesian neural network.}}{10}{figure.2.1}\protected@file@percent }
\newlabel{fig:dnn_bnn}{{2.1}{10}{Difference between parameter estimation of deterministic neural network and Bayesian neural network}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Variational inference}{11}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Introduction}{11}{subsection.2.2.1}\protected@file@percent }
\newlabel{2.6}{{2.6}{11}{Introduction}{equation.2.2.6}{}}
\newlabel{2.7}{{2.7}{11}{Introduction}{equation.2.2.7}{}}
\citation{graves2011practical}
\citation{srivastava2014dropout}
\citation{srivastava2014dropout}
\newlabel{2.8}{{2.8}{12}{Introduction}{equation.2.2.8}{}}
\newlabel{2.9}{{2.9}{12}{Introduction}{equation.2.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Dropout variational inference}{12}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dropout}{12}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces How dropout works\cite  {srivastava2014dropout}.}}{13}{figure.2.2}\protected@file@percent }
\newlabel{fig:dropout}{{2.2}{13}{How dropout works\cite {srivastava2014dropout}}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Bayesian interpretation of dropout}{13}{section*.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A two layer neural network example of dropout inference, a Bernoulli random variable is imposed on each unit of input layer and hidden layer.}}{14}{figure.2.3}\protected@file@percent }
\newlabel{fig:dropout_inference}{{2.3}{14}{A two layer neural network example of dropout inference, a Bernoulli random variable is imposed on each unit of input layer and hidden layer}{figure.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Approximate distribution}{14}{section*.5}\protected@file@percent }
\newlabel{dropout_form}{{2.10}{15}{Approximate distribution}{equation.2.2.10}{}}
\newlabel{appro_dist_form}{{2.11}{15}{Approximate distribution}{equation.2.2.11}{}}
\newlabel{appro_cond_dist_form}{{2.12}{15}{Approximate distribution}{equation.2.2.12}{}}
\newlabel{appr_expectation}{{2.13}{16}{Approximate distribution}{equation.2.2.13}{}}
\newlabel{appr_covariance}{{2.14}{16}{Approximate distribution}{equation.2.2.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Training objective}{16}{section*.6}\protected@file@percent }
\citation{kingma2013auto}
\citation{gal2016uncertainty}
\citation{williams1992simple}
\citation{kingma2013auto}
\citation{kingma2013auto}
\newlabel{dropout_loss}{{2.15}{17}{Training objective}{equation.2.2.15}{}}
\newlabel{dropout_grad}{{2.16}{17}{Training objective}{equation.2.2.16}{}}
\newlabel{elbo_grad}{{2.17}{17}{Training objective}{equation.2.2.17}{}}
\@writefile{toc}{\contentsline {subparagraph}{Re-parameterization trick:}{17}{section*.7}\protected@file@percent }
\newlabel{repa}{{2.18}{18}{Re-parameterization trick:}{equation.2.2.18}{}}
\newlabel{repa1}{{2.19}{18}{Re-parameterization trick:}{equation.2.2.19}{}}
\@writefile{toc}{\contentsline {subparagraph}{KL condition:}{19}{section*.8}\protected@file@percent }
\newlabel{appro_dist_guassian}{{2.20}{19}{KL condition:}{equation.2.2.20}{}}
\newlabel{kl_condition}{{2.21}{19}{KL condition:}{equation.2.2.21}{}}
\newlabel{KL_grad}{{2.22}{20}{KL condition:}{equation.2.2.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Marginalization in testing}{20}{section*.9}\protected@file@percent }
\newlabel{marginalization_test}{{2.23}{20}{Marginalization in testing}{equation.2.2.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Concrete dropout and Multi-Drop}{20}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Concrete dropout}{20}{section*.10}\protected@file@percent }
\citation{srivastava2014dropout}
\citation{gal2017concrete}
\citation{kingma2013auto}
\citation{maddison2016concrete}
\citation{jang2016categorical}
\citation{maddison2016concrete}
\citation{maddison2014sampling}
\@writefile{toc}{\contentsline {subparagraph}{Re-parameterization of Bernoulli distribution}{21}{section*.11}\protected@file@percent }
\citation{maddison2016concrete}
\citation{maddison2016concrete}
\citation{maddison2016concrete}
\citation{maddison2016concrete}
\newlabel{gumbel_max}{{2.2.3}{22}{Re-parameterization of Bernoulli distribution}{section*.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Example of Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$ \cite  {maddison2016concrete}.}}{22}{figure.2.4}\protected@file@percent }
\newlabel{fig:gumbel_max}{{2.4}{22}{Example of Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$ \cite {maddison2016concrete}}{figure.2.4}{}}
\newlabel{gumbel_softmax}{{2.24}{22}{Re-parameterization of Bernoulli distribution}{equation.2.2.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of continuous approximation to Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$\cite  {maddison2016concrete}.}}{23}{figure.2.5}\protected@file@percent }
\newlabel{fig:gumbel_softmax}{{2.5}{23}{Example of continuous approximation to Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$\cite {maddison2016concrete}}{figure.2.5}{}}
\newlabel{bern_repa}{{2.25}{23}{Re-parameterization of Bernoulli distribution}{equation.2.2.25}{}}
\@writefile{toc}{\contentsline {subparagraph}{Dropout regularization}{23}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces One sample and average value of 100 samples drawn from continuous approximation of Bernoulli distribution with parameter $p = [0.1, 0.2, ..., 1.0]$ and temperature $ \lambda =0.1$.}}{24}{figure.2.6}\protected@file@percent }
\newlabel{fig:bern_repa}{{2.6}{24}{One sample and average value of 100 samples drawn from continuous approximation of Bernoulli distribution with parameter $p = [0.1, 0.2, ..., 1.0]$ and temperature $ \lambda =0.1$}{figure.2.6}{}}
\newlabel{KL_grad_concrete}{{2.26}{24}{Dropout regularization}{equation.2.2.26}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-Drop}{24}{section*.13}\protected@file@percent }
\newlabel{KL_grad_multi}{{2.27}{25}{Multi-Drop}{equation.2.2.27}{}}
\newlabel{fig:cdp_dropout1}{{2.2.3}{25}{Multi-Drop}{equation.2.2.27}{}}
\citation{he2016deep}
\newlabel{fig:cdp_dropout2}{{2.2.3}{26}{Multi-Drop}{equation.2.2.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Changes along epochs of keep probability in training network with concrete dropout layer.}}{26}{figure.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Different dropout rates for different hidden units in multi-drop.}}{26}{figure.2.8}\protected@file@percent }
\newlabel{fig:multi-drop}{{2.8}{26}{Different dropout rates for different hidden units in multi-drop}{figure.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Modified network architecture}{26}{subsection.2.2.4}\protected@file@percent }
\citation{srivastava2014dropout}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Modified network architecture.}}{27}{figure.2.9}\protected@file@percent }
\newlabel{fig:modified_net}{{2.9}{27}{Modified network architecture}{figure.2.9}{}}
\citation{bishop2006pattern}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Laplace approximation}{28}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Introduction}{28}{subsection.2.3.1}\protected@file@percent }
\newlabel{point estimate}{{2.28}{28}{Introduction}{equation.2.3.28}{}}
\newlabel{taylor expansion}{{2.29}{28}{Introduction}{equation.2.3.29}{}}
\newlabel{gaussian form}{{2.30}{29}{Introduction}{equation.2.3.30}{}}
\newlabel{expected hessian}{{2.31}{29}{Introduction}{equation.2.3.31}{}}
\citation{ritter2018scalable}
\citation{martens2015optimizing}
\@writefile{toc}{\contentsline {paragraph}{Fisher information matrix}{30}{section*.14}\protected@file@percent }
\newlabel{Fisher}{{2.32}{30}{Fisher information matrix}{equation.2.3.32}{}}
\newlabel{fisher hessian}{{2.33}{30}{Fisher information matrix}{equation.2.3.33}{}}
\newlabel{fisher hessian_with_prior}{{2.34}{30}{Fisher information matrix}{equation.2.3.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Scalable Laplace approximation for neural network}{30}{subsection.2.3.2}\protected@file@percent }
\newlabel{fisher_appr}{{2.37}{31}{Scalable Laplace approximation for neural network}{equation.2.3.37}{}}
\citation{gupta1999matrix}
\newlabel{mvg}{{2.38}{32}{Scalable Laplace approximation for neural network}{equation.2.3.38}{}}
\newlabel{mvg_prior}{{2.39}{32}{Scalable Laplace approximation for neural network}{equation.2.3.39}{}}
\@setckpt{BayesianNeuralNetwork}{
\setcounter{page}{33}
\setcounter{equation}{39}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{0}
\setcounter{StandardModuleDepth}{0}
\setcounter{AM@survey}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{16}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{section@level}{2}
}
