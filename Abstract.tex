\switchlanguage{en} % The abstract is supposed to be in English!

\thispagestyle{plain}

\section*{Abstract}
Deep learning based classifiers have achieved tremendous success on different tasks. However, this kind of classifier cannot provide reliable uncertainty estimation about their predictions and express excessive overconfidence, which means that the models are highly certain although they are not familiar with the related input. This behavior can easily lead to severe consequences in safety-critical applications such as diagnosis of diseases, perception of self-driving car and so on. Especially in the case of the deployment of robots, the robots should be aware of the correctness of the predictions they make, so that they are able to avoid unnecessary accidents and adapt itself in an unfamiliar environment. Besides uncertainty for independent data examples, uncertainty related to dependencies between data examples should be taken into account. When recognizing objects with similar appearances but different contexts in a specific scene, the classifier should express reliably high uncertainty about its predictions because of appearance ambiguities. At the same time, contextual information should be handled properly and utilized to resolve this kind of ambiguities, aiming to further improve the performance. 

In this work, Bayesian neural networks with different inference techniques such as dropout variational inference and scalable Laplace approximation are employed to improve the uncertainty estimation. With a reliable uncertainty estimation, a classifier trained on an easily obtainable dataset should be able to adapt itself in a test environment by collecting dataset for fine-tuning with as little manual efforts as possible. Additionally, a conditional random field is employed to further improve the performance by utilizing more information brought by better uncertainty estimation and handling the contextual information between objects properly. Extensive experiments are performed to show that uncertainty estimation can be improved with Bayesian neural networks in terms of different evaluation metrics. Besides, it is demonstrated that manual efforts in collecting dataset for fine-tuning a classifier can be reduced with the help of improved uncertainty estimation on experiments with two different datasets. Last but not least, experiments also show that the conditional random field is able to further improve the performance by making use of better uncertainty estimation and contextual information.

Deep learning based classifiers have achieved tremendous success on different tasks. 
However, this kind of classifiers cannot provide a reliable uncertainty estimation and express excessive overconfidence, which means that the models are always highly certain even on unfamiliar data.
This behavior can easily lead to severe consequences in safety-critical applications such as diagnosis of diseases, perception of self-driving car or robotics. 
Robots should be aware of the correctness of their predictions to prevent unnecessary accidents and/or to detect failure cases.
Furthermore, this kind of introspection can help the robot to recognize unfamiliar environments and to adapt adequately.
%Besides uncertainty for independent data example, uncertainty related to dependencies between data examples should be taken into account.
While uncertainties of independent data examples can be tackled by fine-tuning fairly easy, uncertainties related to dependencies between data examples cannot be solved straight-forward and additional information has to be incorporated.
Given a classifier with good uncertainty estimations the classification of objects with similar appearances should result in a high uncertainty about its prediction due to the appearance ambiguity. To overcome this issue, contextual information can be used to resolve the ambiguities and improve the initial prediction.

In this work, the uncertainty estimation of deep neural networks are improved by employing Bayesian neural networks. 
Within this context the applicability and performance of different inference techniques such as dropout variational inference and scalable Laplace approximation are evaluated.
The resulting network is applied to a continuous learning use-case where the improved uncertainty estimation is used to select data samples for fine-tuning in a semi-supervised (automatically and manually labeling) manner.
For further improvements a conditional random field is employed to fuse the initial predictions with contextual information between objects.
%Additionally, conditional random field is employed to improve the performance further by utilizing more information brought by better uncertainty estimation and handling the contextual information between objects properly. 
Extensive experiments are performed to show that the uncertainty estimation can be improved with Bayesian neural network in terms of different evaluation metrics.
Besides, it is experimentally shown that manual efforts for collecting a dataset for fine-tuning a classifier can be reduced with the help of improved uncertainty estimation on two benchmark datasets. 
Last but not least, experiments also show that the conditional random field is able to further improve the performance by incorporating contextual information.

\switchlanguage{\lang} % Switch back to the document's default language.
