\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{gal2016dropout}
\citation{ritter2018scalable}
\citation{gal2017concrete}
\citation{gal2016dropout}
\citation{ritter2018scalable}
\citation{gal2017concrete}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theoretical Background}{11}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Bayesian neural network}{11}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Introduction}{12}{subsection.2.1.1}\protected@file@percent }
\newlabel{2.1}{{2.1}{12}{Introduction}{equation.2.1.1}{}}
\newlabel{2.2}{{2.2}{12}{Introduction}{equation.2.1.2}{}}
\newlabel{2.3}{{2.3}{13}{Introduction}{equation.2.1.3}{}}
\newlabel{2.4}{{2.4}{13}{Introduction}{equation.2.1.4}{}}
\newlabel{2.5}{{2.5}{13}{Introduction}{equation.2.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Difference between parameter estimation of deterministic neural network and Bayesian neural network. \leavevmode {\color  {green}explain the notation again here... normally a figure should be understandable only reading the caption } \leavevmode {\color  {green}crop the image so the margin between image and caption is not so big}}}{15}{figure.2.1}\protected@file@percent }
\newlabel{fig:dnn_bnn}{{2.1}{15}{Difference between parameter estimation of deterministic neural network and Bayesian neural network. \maxcom {explain the notation again here... normally a figure should be understandable only reading the caption } \maxcom {crop the image so the margin between image and caption is not so big}}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Dropout variational inference}{15}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Introduction}{15}{section*.9}\protected@file@percent }
\newlabel{2.6}{{2.6}{16}{Introduction}{equation.2.1.6}{}}
\newlabel{2.7}{{2.7}{16}{Introduction}{equation.2.1.7}{}}
\citation{graves2011practical}
\citation{srivastava2014dropout}
\citation{srivastava2014dropout}
\newlabel{2.8}{{2.8}{17}{Introduction}{equation.2.1.8}{}}
\newlabel{2.9}{{2.9}{17}{Introduction}{equation.2.1.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout variational inference}{17}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dropout\cite  {srivastava2014dropout}}{17}{section*.11}\protected@file@percent }
\citation{srivastava2014dropout}
\citation{srivastava2014dropout}
\@writefile{toc}{\contentsline {paragraph}{Dropout}{18}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Schematic illustration of dropout in a three-layer fully connected neural network. (a) the standard neural network with all neurons switched on. (b) after applying dropout only a subset of neurons are used\cite  {srivastava2014dropout}.}}{18}{figure.2.2}\protected@file@percent }
\newlabel{fig:dropout}{{2.2}{18}{Schematic illustration of dropout in a three-layer fully connected neural network. (a) the standard neural network with all neurons switched on. (b) after applying dropout only a subset of neurons are used\cite {srivastava2014dropout}}{figure.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Bayesian interpretation of dropout:}{18}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces An example of two layer neural network with dropout, a Bernoulli random variable is imposed on each unit of input layer and hidden layer.}}{20}{figure.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A two-layer neural network example of dropout inference, a Bernoulli random variable is imposed on each unit of the input layer and the hidden layer.}}{20}{figure.2.4}\protected@file@percent }
\newlabel{fig:dropout_inference}{{2.4}{20}{A two-layer neural network example of dropout inference, a Bernoulli random variable is imposed on each unit of the input layer and the hidden layer}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subparagraph}{Approximate distribution:}{20}{section*.14}\protected@file@percent }
\newlabel{dropout_form}{{2.10}{21}{Approximate distribution:}{equation.2.1.10}{}}
\newlabel{appro_dist_form}{{2.11}{21}{Approximate distribution:}{equation.2.1.11}{}}
\newlabel{appro_cond_dist_form}{{2.12}{22}{Approximate distribution:}{equation.2.1.12}{}}
\newlabel{appr_expectation}{{2.13}{22}{Approximate distribution:}{equation.2.1.13}{}}
\newlabel{appr_covariance}{{2.14}{22}{Approximate distribution:}{equation.2.1.14}{}}
\@writefile{toc}{\contentsline {subparagraph}{Training objective:}{23}{section*.15}\protected@file@percent }
\newlabel{dropout_loss}{{2.15}{23}{Training objective:}{equation.2.1.15}{}}
\citation{kingma2013auto}
\citation{gal2016uncertainty}
\citation{williams1992simple}
\citation{kingma2013auto}
\citation{kingma2013auto}
\citation{kingma2013auto}
\citation{gal2016uncertainty}
\newlabel{dropout_grad}{{2.16}{24}{Training objective:}{equation.2.1.16}{}}
\newlabel{elbo_grad}{{2.17}{24}{Training objective:}{equation.2.1.17}{}}
\@writefile{toc}{\contentsline {subparagraph}{Re-parameterization trick:}{24}{section*.16}\protected@file@percent }
\citation{williams1992simple}
\citation{kingma2013auto}
\citation{kingma2013auto}
\@writefile{toc}{\contentsline {subparagraph}{Re-parameterization trick:}{25}{section*.17}\protected@file@percent }
\newlabel{repa}{{2.18}{25}{Re-parameterization trick:}{equation.2.1.18}{}}
\newlabel{repa1}{{2.19}{26}{Re-parameterization trick:}{equation.2.1.19}{}}
\@writefile{toc}{\contentsline {subparagraph}{KL condition:}{26}{section*.18}\protected@file@percent }
\citation{gal2016uncertainty}
\newlabel{appro_dist_guassian}{{2.20}{27}{KL condition:}{equation.2.1.20}{}}
\newlabel{kl_condition}{{2.21}{27}{KL condition:}{equation.2.1.21}{}}
\newlabel{KL_grad}{{2.22}{27}{KL condition:}{equation.2.1.22}{}}
\citation{srivastava2014dropout}
\citation{gal2017concrete}
\citation{kingma2013auto}
\citation{maddison2016concrete}
\@writefile{toc}{\contentsline {subparagraph}{Marginalization in testing:}{28}{section*.19}\protected@file@percent }
\newlabel{marginalization_test}{{2.23}{28}{Marginalization in testing:}{equation.2.1.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{Concrete dropout}{28}{section*.20}\protected@file@percent }
\citation{jang2016categorical}
\citation{maddison2016concrete}
\citation{maddison2014sampling}
\citation{maddison2016concrete}
\citation{maddison2016concrete}
\@writefile{toc}{\contentsline {subparagraph}{Re-parameterization for Bernoulli distribution:}{29}{section*.21}\protected@file@percent }
\citation{maddison2016concrete}
\citation{maddison2016concrete}
\newlabel{gumbel_max}{{2.1.2}{30}{Re-parameterization for Bernoulli distribution:}{section*.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$ \cite  {maddison2016concrete}.}}{30}{figure.2.5}\protected@file@percent }
\newlabel{fig:gumbel_max}{{2.5}{30}{Example of Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$ \cite {maddison2016concrete}}{figure.2.5}{}}
\newlabel{gumbel_softmax}{{2.24}{30}{Re-parameterization for Bernoulli distribution:}{equation.2.1.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Example of continuous approximation to Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$\cite  {maddison2016concrete}.}}{30}{figure.2.6}\protected@file@percent }
\newlabel{fig:gumbel_softmax}{{2.6}{30}{Example of continuous approximation to Gumbel-max trick for drawing samples from a discrete distribution whose random variable has 3 states and $\{\alpha _{i}\}_{i=1,2,3}$ as class parameters representing the possibility of occurrence of that class. $\{G_{i}\}_{i=1,2,3}$ are i.i.d Gumbel$(0,1)$\cite {maddison2016concrete}}{figure.2.6}{}}
\newlabel{bern_repa}{{2.25}{31}{Re-parameterization for Bernoulli distribution:}{equation.2.1.25}{}}
\@writefile{toc}{\contentsline {subparagraph}{Dropout regularization:}{31}{section*.22}\protected@file@percent }
\newlabel{KL_grad_concrete}{{2.26}{31}{Dropout regularization:}{equation.2.1.26}{}}
\citation{bishop2006pattern}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces One sample and average value of 100 samples drawn from continuous approximation of Bernoulli distribution with parameter $p = [0.1, 0.2, ..., 1.0]$ and temperature $ \lambda =0.1$.}}{32}{figure.2.7}\protected@file@percent }
\newlabel{fig:bern_repa}{{2.7}{32}{One sample and average value of 100 samples drawn from continuous approximation of Bernoulli distribution with parameter $p = [0.1, 0.2, ..., 1.0]$ and temperature $ \lambda =0.1$}{figure.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Laplace approximation}{32}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Introduction}{32}{section*.23}\protected@file@percent }
\newlabel{point estimate}{{2.27}{33}{Introduction}{equation.2.1.27}{}}
\newlabel{taylor expansion}{{2.28}{33}{Introduction}{equation.2.1.28}{}}
\newlabel{gaussian form}{{2.29}{33}{Introduction}{equation.2.1.29}{}}
\newlabel{expected hessian}{{2.30}{34}{Introduction}{equation.2.1.30}{}}
\@writefile{toc}{\contentsline {paragraph}{Fisher information matrix}{34}{section*.24}\protected@file@percent }
\newlabel{Fisher}{{2.31}{34}{Fisher information matrix}{equation.2.1.31}{}}
\newlabel{fisher hessian}{{2.32}{34}{Fisher information matrix}{equation.2.1.32}{}}
\citation{martens2015optimizing}
\citation{ritter2018scalable}
\newlabel{fisher hessian_with_prior}{{2.33}{35}{Fisher information matrix}{equation.2.1.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{Scalable Laplace approximation for neural network}{35}{section*.25}\protected@file@percent }
\citation{gupta1999matrix}
\newlabel{fisher_appr}{{2.36}{36}{Scalable Laplace approximation for neural network}{equation.2.1.36}{}}
\newlabel{mvg}{{2.37}{36}{Scalable Laplace approximation for neural network}{equation.2.1.37}{}}
\newlabel{mvg_prior}{{2.38}{36}{Scalable Laplace approximation for neural network}{equation.2.1.38}{}}
\citation{koller2009probabilistic}
\citation{lafferty2001conditional}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Conditional random field}{37}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Definition}{37}{subsection.2.2.1}\protected@file@percent }
\newlabel{crf_def}{{2.2.1}{37}{Definition}{subsection.2.2.1}{}}
\newlabel{crf}{{2.39}{38}{Definition}{equation.2.2.39}{}}
\citation{hammersley1968markov}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Simple example of conditional random field.}}{39}{figure.2.8}\protected@file@percent }
\newlabel{crf}{{2.8}{39}{Simple example of conditional random field}{figure.2.8}{}}
\newlabel{crf_potential}{{2.40}{39}{Definition}{equation.2.2.40}{}}
\@writefile{toc}{\contentsline {paragraph}{Parametrization}{39}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pairwise feature}{40}{section*.27}\protected@file@percent }
\newlabel{crf_used}{{2.41}{40}{Pairwise feature}{equation.2.2.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Learning}{40}{subsection.2.2.2}\protected@file@percent }
\newlabel{crf_obj}{{2.42}{40}{Learning}{equation.2.2.42}{}}
\citation{koller2009probabilistic}
\citation{pearl2014probabilistic}
\newlabel{log_partition_func}{{2.43}{41}{Learning}{equation.2.2.43}{}}
\newlabel{crf_nll_opt}{{2.44}{41}{Learning}{equation.2.2.44}{}}
\newlabel{crf_grad}{{2.45}{41}{Learning}{equation.2.2.45}{}}
\newlabel{exp_log_part}{{2.46}{41}{Learning}{equation.2.2.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Inference}{42}{subsection.2.2.3}\protected@file@percent }
\newlabel{message}{{2.47}{42}{Inference}{equation.2.2.47}{}}
\newlabel{belief}{{2.48}{42}{Inference}{equation.2.2.48}{}}
\@setckpt{TheoreticalBackground}{
\setcounter{page}{43}
\setcounter{equation}{48}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{0}
\setcounter{StandardModuleDepth}{0}
\setcounter{AM@survey}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{15}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{@todonotes@numberoftodonotes}{6}
\setcounter{section@level}{2}
}
