\chapter{Introduction}
\pagenumbering{arabic}%Ab hier, werden arabische Zahlen benutzt
\setcounter{page}{1}%Mit Abschnitt 1 beginnt die Seitennummerierung neu.
\thispagestyle{empty}

\section{What is uncertainty?}
This question seems a little philosophical and difficult to answer. However, in the context of modeling in machine learning, I find one answer reasonable and illustrative, that is "Uncertainty arises because of limitations in our ability to observe the world, limitations in our ability to model it, and possibly even because of innate nondeterminism."\cite{koller2009probabilistic}. In spite of innate nondeterminism, uncertainty can be further categorized into two main types :\textbf{epistemic uncertainty} and \textbf{aleatoric uncertainty} \cite{der2009aleatory}\cite{senge2014reliable} \cite{kendall2017uncertainties}. 

The former one, epistemic or model uncertainty illustrates the uncertainty in modeling, which is associated with \textbf{imperfect models} of the real world because of insufficient or imperfect knowledge of reality, which corresponds to our ability of modeling, and thus refers to uncertainty caused by a lack of knowledge, i.e., it refers to the epistemic state of the decision maker. Therefore it may be reduced through collecting more knowledge. Examples for this kind of uncertainty can be uncertainty of model parameter, or uncertainty of model structure depending on which kind of model and so on. One simple example to illustrate this kind of uncertainty would be linear regression with limited observed points, where we can use different curves to explain those points and then predict unobserved points with those curves. If we have observed enough points in some intervals, the models or curves that are able to explain the data are more restricted and if there are not enough points, there would be more curves that can explain these points, which indicates high uncertainty. In reality, it's quite normal to have inadequate data, and thus difficult to model the underlying distribution quite well and precisely. Therefore we could explain the data with several or many different models, where the model uncertainty comes in.

The latter one, aleatoric or data uncertainty, accounts for the \textbf{inherent randomness} of underlying phenomenon which is expressed as variability in observed data and meanwhile associated with the limitation of our observing ability. It's treated as non-reducible within our ability to observe the data. Examples for this kind of uncertainty could be noise induced by limited resolution of sensor or ambiguity from content of data which are unrelated to the model we use to represent these data. One example can be classification of images captured by cameras with different resolutions, images with lower resolutions would have higher uncertainty because they are more ambiguous and less illustrative compared with those with higher resolutions. 

In other words, epistemic uncertainty refers to the reducible part of the (total) uncertainty, whereas aleatoric uncertainty
refers to the non-reducible part. Model uncertainty and aleatoric uncertainty are combined and expressed in the final prediction, which is so called predictive uncertainty.

\section{Why do we need uncertainty?}
Aforementioned model uncertainty represents the belief of model about its predictions and thus bring us more useful information about predictions related with model and observed data instead of just a crispy prediction, whose predictor, is always overconfident when advanced neural network architectures are employed\cite{guo2017calibration}. As far as I am concerned, this kind of uncertainty plays an important role in two following scenarios. 

The first one are those requiring \textbf{high safety guarantee} or \textbf{optimal decision-making}, in which false predictions can cause hazardous consequences, which is so called AI safety\cite{amodei2016concrete}. With reliable model uncertainty estimation, we are able to know when the model is uncertain about its prediction and then people or operator are aware of that and can take corresponding counter measures in order to avoid unnecessary accidents. With more and more wide-spread deep learning algorithms in real life applications, concrete examples for this case include steering control in self-driving car\cite{mcallister2017concrete}, disease diagnosis in medical domain\cite{leibig2017leveraging}, or even tasks in aerospace or other domains requiring higher precision and stronger robustness and so on. Although someone would argue that if the trained model can perform perfectly, that means that it is able to achieve 100\% accuracy, then we do not need this kind of uncertainty information. However, as mentioned in \cite{denker1991transforming}, it's hard or even impossible to have enough training data to define a precise model, which is in coincidence with reality because it's hard to define a task very precisely and get access to enough samples drawn from its real distribution. Uncertainty information would be more valuable and important when algorithms are confronted with multiple tasks\cite{kendall2017multi}. Other relevant tasks such as misclassified and out-of-distribution data detection\cite{hendrycks2016baseline} and adversarial attacks \cite{kurakin2016adversarial}\cite{feinman2017detecting} have also attracted more and more research interests in uncertainty estimation in order to address these challenges.

The second one are scenarios requiring \textbf{interaction between algorithms and people or environments}. Model uncertainty can build a bridge between machine learning algorithms and human beings which can construct a more robust and more data-efficient machine learning system. This scenario involves different kinds of tasks such as active learning\cite{gal2017deep}, reinforcement learning\cite{blundell2015weight}\cite{osband2016deep}\cite{gal2016improving}, automatically labeling, industrial components inspection and so on. The idea behind that is, with reliable model uncertainty estimation we know which data sample or prediction the model is unfamiliar with. In active learning or reinforcement learning, those unfamiliar data samples can be used to train our model more quickly and efficiently. In tasks involving human robot interaction, we can know when the model requires help from human experts based on uncertainty estimation. For example, in industrial component inspection task, we firstly train a model which can perform quite well on some evaluation datasets and then deploy them in real applications. At this moment, we assume that all predictions made by the model are true all the time. However, there is no guarantee that this model has learned to represent the real distribution of data perfectly and is robust against slight or even large domain transfer. If the distributions of the real world data vary because of unexpected factors such equipments aging or changes of weather condition. The model would fail silently and lead to unexpected accidents. With reliable model uncertainty estimation, this kind of weakness can be mitigated or even eliminated. 

On the other hand, aleatoric uncertainty mainly assist in \textbf{improving model performance} by quantifying and considering this kind of information by representing noisy level of predictions and making good use of data points with less noise. By taking this information into account, the model can be trained more optimally and yield better performance, some examples of image data and Lidar data can be referred to \cite{kendall2016modelling}\cite{feng2018towards}. Instead of representing noise, data uncertainty also includes uncertainty from semantic view point, which we can also incorporate into training and yield more robust model. This kind of uncertainty is not quantified explicitly but can be incorporated conceptually with help of specifically designed model and high level statistics cues or features to \textbf{disambiguate prediction} directly. One example of this can be modeling contextual information among pixels with conditional random field in semantic segmentation task\cite{krahenbuhl2011efficient}\cite{sutton2012introduction}\cite{lin2016efficient}. The idea behind that is simple and conditional random field provides a principle way to achieve this idea. Since uncertainty caused by appearance ambiguity represents strong correlation between different categories. Conditional random field can model correlation or dependency between random variables by taking into account high order potentials. In semantic segmentation task, because of the reason of tractable computation, at most second order potentials are taken into account and can yield better performance compared results with just first order potentials.

\newpage

\section{How can we obtain and handle uncertainty in deep learning?}
As we know, deep learning is a powerful black box model while it has achieved tremendous success in different tasks. Therefore to obtain and handle uncertainty in deep learning model and data can help us to get more understanding about this black box model and make deep learning based application more robust and safer. 

Regarding model uncertainty or quality measure that acts similar to model uncertainty, there are many different ways to obtain this kind of quality measure of prediction. On the one hand, because of intractability of real model uncertainty in deep neural networks, there are some \textbf{ad-hoc approaches} that try to obtain such a quality measure on specific tasks which are solved usually with model uncertainty such as misclassified and out-of-distribution(OOD) detection task. \cite{liang2017enhancing} combines temperature scaling and input preprocessing which is called adversarial training together and yields a simple but effective out-of-distribution sample detector, but this approach requires additional out-of-distribution dataset for training which is hard to collect in reality. \cite{devries2018learning} modifies the loss function to train a classifier against OOD data, however this approach is hard to generalize in case of slight domain transfer. \cite{lee2017training} treats uniform distribution as ground truth distribution of OOD data and show that OOD data can be generated via generative adversarial network(GAN). This approach seems practical, but training GAN effectively does not always work for images of large size because of high dimension of data distribution. 

On the other hand, there are some more theoretically sound approaches which employ \textbf{Bayesian neural network}\cite{mackay1992practical}\cite{neal2012bayesian}, \textbf{bootstrap}\cite{osband2016deep} or \textbf{ensemble} method\cite{lakshminarayanan2017simple} or even ensemble of Bayesian neural network\cite{smith2018understanding}. Although Bayesian neural network provides a principal way to obtain model uncertainty by putting randomness on model parameters, it's difficult to infer the exact posterior distribution over model parameters of deep neural network because of its complex architecture and large scale dataset. Therefore approximate inference plays an important role in addressing this issue. The golden standard for approximate inference is Hamiltonian Monte Carlo\cite{neal2012bayesian}, which requires dealing with whole batch training data and storing the samples of posterior distribution. While the former issue is addressed partially by stochastic gradient Langevin Dynamics\cite{welling2011bayesian}, the latter one attracts many research interests like \cite{balan2015bayesian}\cite{wang2018adversarial}. However, since it's a Markov Chain Monte Carlo(MCMC) method, to assess the convergence is still non-trivial.
 
There are some alternatives to MCMC, one popular one is variational inference(VI)\cite{hinton1993keeping}. In VI, this inference problem is cast into optimization problem by minimizing the Kullbach-Leibler divergence between the approximate posterior distribution and real posterior distribution. Therefore approximate distribution family needs to be chosen and thus has a large impact on the result, which may impede the performance if the chosen distribution family is not flexible enough for the problem. \cite{graves2011practical} firstly employed fully factorized Gaussian with a biased gradient estimator on a practical problem. Later \cite{blundell2015weight} improved the result with the same approximate distribution but different estimator with help of "reparameterization trick"\cite{kingma2013auto}. \citealp{hernandez2015probabilistic} also used the same approximate distribution but with expectation propagation\cite{minka2001expectation} instead of variational inference.
Since the fully factorization assumption may impose an restriction on the flexibility of the approximate distribution between the correlations between weights could not be learned and expressed. By taking this into account, there are many attempts trying to use more expressive approximate distribution such as Bernoulli (treated as mixture of two Gaussian with small variance)\cite{gal2016dropout} and Gaussian \cite{kingma2015variational} which capture variances of rows in weight matrix. Another more expressive one is matrix variate Gaussian distribution\cite{louizos2016structured}\cite{sun2017learning}\cite{zhang2017noisy}, which capture both rows and columns correlation in weight matrix. Additionally inspired by the idea of normalizing flow in latent variable models \cite{louizos2017multiplicative}applied normalizing
flows to auxiliary latent variables to produce more
flexible approximate posteriors. 


Another alternative is Laplace approximation\cite{mackay1992practical}, which put a Gaussian centered at MAP estimate of model parameters with a covariance determined by the inverse of Hessian of the log-likelihood. The main issue of this approach for the large neural network is the large size of Hessian which is quadratic of the number of parameters and inversion operation on it whose complexity is cubic over number of elements of Hessian matrix. Recently \cite{ritter2018scalable} working out this problem by approximating the Hessian or the Fisher matrix of log-likelihood with kronecked-factorization which reduces the storage size and complexity of inversion operation. The resulting posterior can also be treated as matrix variate Gaussian.  

However, most of those ideas are verified with a relatively simple architecture and on toy or small-scale datasets and need to be modified a lot in order to serve the existing advanced architectures, which is undesirable in our work and many practical applications. Therefore dropout inference\cite{gal2016dropout} looks promising considering this restriction and thus is adopted to obtain model uncertainty in our work. On the other hand, scalable Laplace approximation \cite{ritter2018scalable} has the potential resolving this issue, because it requires only one MAP point estimate of model parameters, so there is no need to modify the training phase which could save a lot of computation effort and unnecessary modifications for many existing architectures.

When it comes to aleatoric uncertainty, as noise of data itself, it can be modeled by adding another head on top of the network \cite{kendall2017uncertainties}. However, the uncertainty caused by appearance ambiguity is not trivial to include in deep neural network training. Therefore another model, \textbf{conditional random field}\cite{lafferty2001conditional}, is adopted based on the predictions of discriminative neural network classifier. As is mentioned in previous section, choice of features of second order potentials is key factor in this problem. In this work, because we work on scene object recognition task, therefore the correlation between objects in specific scene are of importance. The correlation can be represented by co-occurrence matrix co-occurrence\cite{ladicky2010graph}\cite{rasiwasia2009holistic}\cite{galleguillos2008object}\cite{rabinovich2007objects}.

\section{Contributions and Structure}
This work can be divided into two main parts, both focusing on uncertainty-based improvement for a deep learning based classifier on object recognition task. 

The first part is to employ dropout inference to obtain reliable and calibrated uncertainty from classifier with ResNet50\cite{he2016deep} as backbone. We evaluate the prediction and calibration performance of this probabilistic classifier and then based on uncertainty estimation, we select predictions with low uncertainty/high confidence as automatically labeled data which is used for training a more accurate classifier while the performance of original one declines a lot in case of slight domain transfer which is caused by possibly by light condition changes, appearance changes and even recording equipment changes. This is proof-of-concept idea which tries to proof that improved uncertainty estimation can assist in automatic labeling and reducing manual effort in fine-tuning a new classifier in case of slight domain transfer during deployment of robot. In this part, we consider advanced version of dropout inference and its variants for improving the results both in terms of accuracy and calibration. 

The second part is to handle the uncertainty caused by appearance ambiguity. To note that again, this kind of uncertainty is not modeled directly, but the concept is taken into account in choosing model and features. To achieve this goal, we choose conditional random field to model the correlation between objects with similar appearance but different semantics. As key factor of the model, we employ co-occurrence statics of categories as features of second order potentials.

There are several main contributions of this work:
\begin{itemize}
 \item to show that dropout inference and Laplace approximation can obtain better uncertainty information on multi-class classification problem with 51 classes, which can be further improved by learning dropout rates during training and its ensemble.
 
 \item to show that manual effort in data collection in slight domain transfer learning task can be reduced by making use of reliable uncertainty estimation.
 
 \item to show that uncertainty caused by appearance ambiguity can be resolved by integrating contextual information via condition random field.
 
 
\end{itemize}

The structure of this work is as follows, in order to give reader a clear theoretical background of models employed in this work, I will briefly review the basic theory and ideas of Bayesian neural network as well as dropout inference and Laplace approximation in chapter 2, where detailed adaptations and modifications of original model are described. In chapter 3, similar to previous chapter, the theory of conditional random field is reviewed firstly and the adaptations are described. In chapter 4, different experiments and results about evaluation of different variants of dropout inference are given and discussed firstly. In the following, experiments and results on scene recognition task resolved by conditional random field with co-occurrence statics as second order features are also showed and discussed. Then conclusion and possible future work come in the chapter 5.