\chapter{Introduction}
\pagenumbering{arabic}%Ab hier, werden arabische Zahlen benutzt
\setcounter{page}{1}%Mit Abschnitt 1 beginnt die Seitennummerierung neu.
\thispagestyle{empty}

\section{What is uncertainty?}
This question seems a little philosophical and difficult to answer. However, in the context of modeling in machine learning, one reasonable and illustrative answer is: "Uncertainty arises because of limitations in our ability to observe the world, limitations in our ability to model it, and possibly even because of innate nondeterminism."\cite{koller2009probabilistic}. In spite of innate nondeterminism, uncertainty can be further categorized into two main types: \textbf{epistemic uncertainty} and \textbf{aleatoric uncertainty} \cite{der2009aleatory, senge2014reliable, kendall2017uncertainties}. 

%The former one, epistemic or model uncertainty illustrates the uncertainty in modeling, which is associated with \textbf{imperfect models} of the real world due to insufficient or imperfect knowledge of reality, which corresponds to our ability of modeling, and thus refers to uncertainty caused by a lack of knowledge, i.e., it refers to the epistemic state of the decision maker. 
The former one, epistemic or model uncertainty illustrates the uncertainty in modeling, associated with \textbf{imperfect models} of the real world due to insufficient or imperfect knowledge of reality, i.e., it refers to the epistemic state of the decision maker.
Therefore it can reduced by collecting more knowledge. % Among this uncertainty type are the uncertainty of model parameters, or the uncertainty of the model structure.  depending on which kind of model and so on. 
One simple example to illustrate this kind of uncertainty is a linear regression with a limited number of observed points, where different curves are obtained during training and then used to predict unobserved points. After observing enough points in some intervals, the generated models or curves are more restricted, hence the uncertainty decreases. On the other hand, if there are insufficient points in specific intervals, upon which large number of curves can be obtained to fit them during training, which induces high model uncertainty.  In reality, it is common to have inadequate amount of data, and thus hard to model the underlying distribution well and precisely. Therefore the observed data can be explained by a large variety of different models, from where model uncertainty comes.

The latter one, aleatoric or data uncertainty, accounts for the \textbf{inherent randomness} of an underlying phenomenon which is expressed as variability in the observed data and meanwhile associated with the limitation of our observing ability. It is treated as non-reducible within our ability to observe the data. Examples for this kind of uncertainty are noise induced by limited resolution of sensors or ambiguity from the content of data which are unrelated to the model used to represent the data. One simple example can be classification of images captured by cameras with different resolutions. Images with lower resolutions have higher uncertainty because they are more ambiguous and less illustrative compared to those with higher resolutions. 

In other words, epistemic uncertainty refers to the reducible part of the (total) uncertainty, whereas aleatoric one refers to the non-reducible part. When testing, model and aleatoric uncertainty, are combined together and expressed in the final prediction, which is so-called \textbf{predictive uncertainty}.

\section{Why do we need uncertainty?}
The aforementioned model uncertainty represents the belief of a model about its predictions, which brings additional information about randomness induced by the model compared with deterministic counter part, which provides only a hard prediction. This kind of uncertainty plays an important role in the two following scenarios. 

The first use-cases are those requiring \textbf{high safety guarantee} or \textbf{optimal decision-making}, in which false predictions can cause hazardous consequences, which is so called AI safety\cite{amodei2016concrete}. Reliable model uncertainty estimation can equip the model with ability to know when the it does not know. More than that, people or operator can take corresponding counter measures to avoid unnecessary accidents. With more and more (deep) learning algorithms applied in real life applications which also incorporate the human, this kind of ability plays an more and more important role. Concrete examples include steering control in self-driving cars\cite{mcallister2017concrete}, disease diagnosis in the medical domain\cite{leibig2017leveraging}, or even applications in aerospace or other domains requiring higher precision and stronger robustness. One could argue that if the trained model can perform perfectly, meaning to achieve 100\% accuracy, the need of the uncertainty information decreases. 
%However, as mentioned in \cite{denker1991transforming}, it is \maxcom{in a objective work you write it is instead of it'S ;)} hard or even impossible to have enough training data to define a precise model, which is in coincidence with reality because it is hard to define a task very precisely and get access to enough samples drawn from its real distribution. 
% However, as already mentioned, every generated model underlies to a certain degree the epistemic as well as the data uncertainty.
\cite{denker1991transforming} argue that, it is hard or even impossible to have enough training data to define a precise model, which is in coincidence with reality because it is hard to define a task very precisely and get access to enough samples drawn from its real distribution. Uncertainty information is even more valuable and important when algorithms are confronted with multiple tasks\cite{kendall2017multi}. Other relevant tasks such as misclassified 
%\todo{well, you are already talking about misclassification above right? so leave it out here?} 
and out-of-distribution data detection\cite{hendrycks2016baseline} and adversarial attacks \cite{kurakin2016adversarial, feinman2017detecting} 
%\todo{I would cite more references together like I did it here. What do you think?} \maxcom{so if you like this kind of citation, change them in the whole work ;) By now, I leave it as it is ok?}
have also attracted more and more research interests in uncertainty estimation, aiming to address these challenges.

The second scenario are cases requiring \textbf{interaction between algorithms and people or environments}. Model uncertainty can be reduced by build a bridge between machine learning algorithms and humans. Such interaction or the exploration of the environment can establish a more robust and more data-efficient machine learning system. This scenario involves different kind of fields such as active learning\cite{gal2017deep}, reinforcement learning\cite{blundell2015weight}\cite{osband2016deep}\cite{gal2016improving} %\maxcom{reinforcement learning does not really demand a human interaction...I added a part to the sentence before, so that also reinforcement learning is correct. is it fine like this?}
, automatic labeling, industrial components inspection 
%\maxcom{why does this require human interaction? isn't this just a use case for learning?} 
and so on. The idea behind that is, with reliable model uncertainty estimation we know which data sample the model is unfamiliar with and thus providing more information for the model 
%\maxcom{this sentence relies more to the out-of-distribution no?}
. In active or reinforcement learning, theses unfamiliar data samples can be used to improve the corresponding model more efficiently. In tasks involving human robot interaction, the uncertainty estimation indicates when the model requires help from human experts. For instance, in industrial component inspection task, firstly a model which can perform quite well on some evaluation datasets is trained. With the assumption that all predictions made by the model are correct, the system is deployed to the real application.
% and deployed them in the real application. At this moment, we assume that all predictions made by the model are true all the time. 
However, there is no guarantee that this model represents the real distribution of data perfectly and is robust against a slight or even large domain gap. If the distributions of the real world data vary a lot because of unexpected factors e.g. equipment aging or changes of weather condition, the model could fail silently and lead to unexpected accidents. With reliable model uncertainty estimation, this kind of weakness can be mitigated or even eliminated. 

On the other hand, aleatoric uncertainty mainly assists in \textbf{improving model performance} by quantifying and considering this kind of information by representing noisy level of predictions and making good use of data points with less noise. By taking this information into account, the model can be trained more optimally and yield a better performance, some examples of image data and Lidar data can be referred to \cite{kendall2016modelling}\cite{feng2018towards}. Instead of representing noise, data uncertainty also includes uncertainty from a semantic view point, which we can be also incorporated into training and yield more robust models. This kind of uncertainty is not quantified explicitly but can be incorporated conceptually with the help of specifically designed models and high level statistic cues or features to \textbf{disambiguate predictions} directly. One example of this is the modeling contextual information among pixels by a so-called Conditional Random Field(CRF) 
%\todo{you do not want to use abbreviations in your work?! check the glossary package ;)} 
in the semantic segmentation task\cite{krahenbuhl2011efficient}\cite{sutton2012introduction}\cite{lin2016efficient}. The idea behind that is simple and CRF provides a principle way to achieve this idea. Since uncertainty caused by the appearance ambiguity represents strong correlation between different categories,CRF can model the correlations or dependencies between random variables by taking into account high order potentials. In the task of semantic segmentation, for tractable computation, at most second order potentials are taken into account and can yield better performance compared results with just first order potentials. 
%\maxcom{this part with the crf and semantic segmentation is a bit confusing, if one does not know how it works. Perhaps you go over the few sentences again...}
\section{How can we obtain and handle uncertainty in deep learning?}
As already known, deep learning is a powerful black box model while it has achieved tremendous success in different tasks. Therefore to obtain and handle uncertainty can help to get a better understanding about this black box model and make deep learning based applications more robust and safer. 

Regarding model uncertainty or quality measure that acts similar to model uncertainty, there are many different ways to obtain this kind of quality measure of prediction. On the one hand, due to the intractability of the real model uncertainty in deep neural networks (DNN), there are some \textbf{ad-hoc approaches} that try to obtain such a quality measure on specific tasks.
% which are solved usually with model uncertainty such as misclassified and out-of-distribution(OOD)\maxcom{you already used OOD before} detection task. 
\cite{liang2017enhancing} combine temperature scaling and input preprocessing which is called adversarial training together and yields a simple but effective OOD sample detector, but this approach requires an additional OOD dataset for training which is hard to collect in reality. \cite{devries2018learning} modify the loss function to train a classifier against OOD data, however this approach is hard to generalize in case of slight domain transfers. \cite{lee2017training} treat a uniform distribution as ground truth distribution of the OOD data and show that OOD data can be generated via a so-called Generative Adversarial Network(GAN). This approach seems practical, but training a GAN effectively does not always work for images because of the high dimensionality of the data distribution. 
%\maxcom{all three works are written by only one person?! otherwise you should leave the S at the end of the verbs}

On the other hand, there are some other more theoretically sound approaches which employ \textbf{Bayesian neural networks}\cite{mackay1992practical}\cite{neal2012bayesian}, \textbf{bootstraping}\cite{osband2016deep}, \textbf{ensemble} methods\cite{lakshminarayanan2017simple} or even ensemble of Bayesian neural networks\cite{smith2018understanding}. Although a Bayesian neural network (BNN) provides a principal way to obtain model uncertainty by considering distribution on model parameters, it is difficult to infer the exact posterior distribution over model parameters of DNN because of its complex architecture and large scale training set. Therefore approximate inference plays an important role in addressing this issue. The golden standard for the approximate inference is Hamiltonian Monte Carlo\cite{neal2012bayesian}, which requires dealing with whole batch training data and storing the samples of posterior distribution. While the former issue is addressed partially by stochastic gradient Langevin Dynamics\cite{welling2011bayesian}(SGLD), the latter one attracts a lot of research interests like \cite{balan2015bayesian}\cite{wang2018adversarial}. However, since it is a Markov Chain Monte Carlo (MCMC) method, to assess the convergence is still non-trivial.

There are some alternatives to MCMC, one popular one is variational inference(VI)\cite{hinton1993keeping}. In VI, this inference problem is cast into optimization problem by minimizing the Kullbach-Leibler divergence (KL-div) between the approximate posterior distribution and real posterior. Therefore approximate distribution family needs to be chosen and thus has a large influence on the performance. \cite{graves2011practical} firstly employed fully factorized Gaussian with a biased gradient estimator on a practical problem. Later \cite{blundell2015weight} improved the result with the same approximate distribution but different estimator with help of "re-parameterization trick"\cite{kingma2013auto}. \citealp{hernandez2015probabilistic} also used the same approximate distribution but with expectation propagation\cite{minka2001expectation} instead of VI.
Since the assumption of fully factorization may impose an strong restriction on the flexibility of the approximate distribution and the correlations between weights can not be captured. By taking this into account, there are many attempts trying to use more expressive approximate distribution such as Bernoulli (treated as mixture of two Gaussian with small variance)\cite{gal2016dropout} and Gaussian with non-diagonal covariance matrix \cite{kingma2015variational} which can capture variances of rows in weight matrices. Another more expressive one is matrix variate Gaussian distribution\cite{louizos2016structured}\cite{sun2017learning}\cite{zhang2017noisy}, which capture both rows and columns correlations in weight matrices. Additionally inspired by the idea of normalizing flow in latent variable models \cite{louizos2017multiplicative}applied normalizing flows to auxiliary latent variables to produce more flexible approximate posteriors with help of "local re-parameterization trick"\cite{kingma2015variational}. 


Another alternative is the Laplace approximation\cite{mackay1992practical}, which puts a Gaussian centered at Maximum a posterior(MAP) estimate of the model parameters with a covariance determined by the inverse of the Hessian matrix of the log posterior distribution. The main issue of this approach for large neural networks is infeasible computation and storage cause by large size of the Hessian matrix which is quadratic of the number of parameters and inversion operation on it whose complexity is cubic over the number of elements of Hessian matrix. Recently \cite{ritter2018scalable} address this problem by approximating the Hessian or the Fisher matrix of log-likelihood with kronecked-factorization approximation which reduces the storage size and complexity of inversion operation. The resulting posterior can also be treated as a matrix variate Gaussian.  

However, most of those ideas are verified with a relatively simple architecture and on toy or small-scale datasets and need to be modified a lot in order to serve the existing advanced architectures, which is undesirable in many practical applications. Therefore the so-called dropout inference\cite{gal2016dropout} looks promising considering this restriction and thus is adopted to obtain model uncertainty in this work. On the other hand, scalable Laplace approximation \cite{ritter2018scalable} has the potential to resolve this issue, because it requires only one MAP point estimate of model parameters, so there is no need to modify the training phase which could save a lot of computation effort and unnecessary modifications for many existing architectures.

When it comes to aleatoric uncertainty, as noise of data itself, it can be modeled by adding another head on top of the network \cite{kendall2017uncertainties}. However, the uncertainty caused by appearance ambiguity is not trivial to include in a deep neural network training. Therefore another model, \textbf{CRF}
%\maxcom{you already introduced CRF before}
\cite{lafferty2001conditional}, is adopted based on the predictions of a discriminative neural network classifier. As mentioned in the previous section, the choice of features of second order potentials is a key factor in this problem. In this work, addressing the scene object recognition task, the correlation between objects in specific scene are of importance. The correlation can be represented by a co-occurrence matrix \cite{ladicky2010graph}\cite{rasiwasia2009holistic}\cite{galleguillos2008object}\cite{rabinovich2007objects}.

\section{Contributions and Structure}
This work can be divided into two main parts, both focusing on uncertainty-based improvement for a deep learning based classifier on object recognition task. 

The first part is to employ BNN to obtain reliable and calibrated uncertainty. In order to keep the ability of powerful feature extraction of DNN, we modify a classifier with ResNet50\cite{he2016deep} as backbone to incorporate different aforementioned inference techniques including dropout variational inference and scalable Laplace approximation. We evaluate prediction and calibration performance of BNN and then select predictions with low uncertainty/high confidence as automatically labeled data based on improved uncertainty estimation. This labeled dataset collected with little manual efforts is used for training a more accurate classifier while the performance of original one drops a lot in case of slight domain transfer caused by possibly by changes of light condition, changes of appearance and even changes of recording equipment. This is proof-of-concept idea which tries to prove that improved uncertainty estimation can assist in automatic labeling and reducing manual effort in fine-tuning a more accurate classifier in case of slight domain transfer during deployment of robots. 

The second part is to handle the uncertainty caused by appearance ambiguity properly. To note that again, this kind of uncertainty is not modeled directly, but the concept is taken into account in choosing model and features. To this end, CRF is selected to capture the contextual relationship between objects with similar appearance but different contexts and at the same time utilize the information brought by better uncertainty estimation obtained in the first part. Key factor to make CRF to work is to design informative pairwise features which should not contradictory to unary feature. In this work, binary co-occurrence matrix of categories as pairwise features are employed.

The main contributions of this work are:
\begin{itemize}
	\item to show that dropout variational inference and scalable Laplace approximation can obtain better uncertainty information from a large and complex network on a multi-class classification problem with 51 classes, which can be further improved by learning dropout rates during training and their ensemble.
	\item to show that manual effort in data collection in a slight domain transfer learning task can be reduced by making use of reliable uncertainty estimation. With help of this, classifier can learn continuously in test environment which has domain gap to the training set.
	\item to show that CRF can improve the results from discriminative classifier by handling uncertainty caused by appearance ambiguity and utilizing information brought by better uncertainty estimation.
\end{itemize}

The structure of this work is as follows. In order to give the reader a clear theoretical background of techniques applied in this work, the basic theory and ideas of Bayesian neural network and inference techniques such as dropout variational inference and scalable Laplace approximation, and the theory of CRF are briefly reviewed in chapter 2. In chapter 3, technical approaches proposed and employed in this work, including proposed variants of concrete dropout, modified network architecture, combination of BNN and CRF as well as pipeline of continuous learning are introduced. In chapter 4, extensive experiments are performed to evaluate performance of uncertainty estimation of BNN with different inference techniques. In the following, experiments on performance of continuous learning and improvements brought by CRF are performed. Then summary of this work, conclusion and possible future work are presented in chapter 5.

%\maxcom{I like the introduction and the related work :) it something new and not as the other works!!! A little bit more robotics could have been in there but that is fine :) it is nicely written just some minor changes (check my comments). Please check the citation type if you want more citations separately or together. and check the abbreviations (e.g. Conditional Random Field -> CRF, Deep Neural Network -> DNN and so on... to check when the first use of the term is used you could use the package glossary which checks this for you :) but cool! looks already professional :) ah and another thing is, I would not write in the first person singular (e.g. I show....) or plural (we show....). so I changed this here already. what do you think?}
