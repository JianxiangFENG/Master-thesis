\chapter{Introduction}
\pagenumbering{arabic}%Ab hier, werden arabische Zahlen benutzt
\setcounter{page}{1}%Mit Abschnitt 1 beginnt die Seitennummerierung neu.
\thispagestyle{empty}

\section{What is uncertainty?}
This question seems a little philosophical and difficult to answer. However, in the context of modeling in machine learning, one reasonable and illustrative answer is: "Uncertainty arises because of limitations in our ability to observe the world, limitations in our ability to model it, and possibly even because of innate nondeterminism."\cite{koller2009probabilistic}. In spite of innate nondeterminism, uncertainty can be further categorized into two main types: \textbf{epistemic uncertainty} and \textbf{aleatoric uncertainty} \cite{der2009aleatory, senge2014reliable, kendall2017uncertainties}. 

%The former one, epistemic or model uncertainty illustrates the uncertainty in modeling, which is associated with \textbf{imperfect models} of the real world due to insufficient or imperfect knowledge of reality, which corresponds to our ability of modeling, and thus refers to uncertainty caused by a lack of knowledge, i.e., it refers to the epistemic state of the decision maker. 
The former one, epistemic or model uncertainty illustrates the uncertainty in modeling, associated with \textbf{imperfect models} of the real world due to insufficient or imperfect knowledge of reality, i.e., it refers to the epistemic state of the decision maker.
Therefore it may be reduced by collecting more knowledge. Among this uncertainty type are the uncertainty of model parameters, or the uncertainty of the model structure.% depending on which kind of model and so on. 
One simple example to illustrate this kind of uncertainty is a linear regression with a limited number of observed points, where different curves to fit those points and then predict unobserved points can be used. After observing enough points in some intervals, the generated models or curves are more restricted, hence the uncertainty decreases. On the other hand an insufficient number of points, leads to a higher variety of applicable curves, which indicates a higher uncertainty.  In reality, it is quite normal to have inadequate amount of data data, and thus difficulty to model the underlying distribution well and precisely. Therefore the observed data can be explained by a large variety of different models, which introduces the model uncertainty.

The latter one, aleatoric or data uncertainty, accounts for the \textbf{inherent randomness} of an underlying phenomenon which is expressed as variability in the observed data and meanwhile associated with the limitation of our observing ability. It is treated as non-reducible within our ability to observe the data. Examples for this kind of uncertainty are noise induced by limited resolution of sensors or ambiguity from the content of data which are unrelated to the used model to represent these data. One example can be classification of images captured by cameras with different resolutions. Images with lower resolutions have higher uncertainty because they are more ambiguous and less illustrative compared to those with higher resolutions. 

In other words, epistemic uncertainty refers to the reducible part of the (total) uncertainty, whereas aleatoric uncertainty refers to the non-reducible part. 
Both types, model uncertainty and aleatoric uncertainty, are combined and expressed in the final prediction, which is defined as predictive uncertainty.

\section{Why do we need uncertainty?}
The aforementioned model uncertainty represents the belief of a model about its predictions and thus brings additional information about the prediction related to the model and the observed data. This kind of uncertainty plays an important role in the two following scenarios. 

The first use-cases are those requiring \textbf{high safety guarantee} or \textbf{optimal decision-making}, in which false predictions can cause hazardous consequences, which is so called AI safety\cite{amodei2016concrete}. With a reliable model uncertainty estimation, the ability to know when the model is uncertain about its prediction is given. Based on this awareness people or the operator can take corresponding counter measures in order to avoid unnecessary accidents. With more and more wide-spread (deep) learning algorithms in real life applications which also incorporate the human, this ability gains more importance. Concrete examples include steering control in self-driving cars\cite{mcallister2017concrete}, disease diagnosis in the medical domain\cite{leibig2017leveraging}, or even applications in aerospace or other domains requiring higher precision and stronger robustness. One could argue that if the trained model can perform perfectly, meaning to achieve 100\% accuracy, the need of the uncertainty information decreases. 
%However, as mentioned in \cite{denker1991transforming}, it is \maxcom{in a objective work you write it is instead of it'S ;)} hard or even impossible to have enough training data to define a precise model, which is in coincidence with reality because it is hard to define a task very precisely and get access to enough samples drawn from its real distribution. 
However, as already mentioned, every generated model underlies to a certain degree the epistemic as well as the data uncertainty.
\cite{denker1991transforming} argue that, it is hard or even impossible to have enough training data to define a precise model, which is in coincidence with reality because it is hard to define a task very precisely and get access to enough samples drawn from its real distribution. Uncertainty information is even more valuable and important when algorithms are confronted with multiple tasks\cite{kendall2017multi}. Other relevant tasks such as misclassified 
%\todo{well, you are already talking about misclassification above right? so leave it out here?} 
and out-of-distribution data detection\cite{hendrycks2016baseline} and adversarial attacks \cite{kurakin2016adversarial, feinman2017detecting} 
%\todo{I would cite more references together like I did it here. What do you think?} \maxcom{so if you like this kind of citation, change them in the whole work ;) By now, I leave it as it is ok?}
have also attracted more and more research interests in uncertainty estimation in order to address these challenges.

The second scenario are cases requiring \textbf{interaction between algorithms and people or environments}. Model uncertainty can be reduced by build a bridge between machine learning algorithms and humans. Such interaction or the exploration of the environment can construct a more robust and more data-efficient machine learning system. This scenario involves different kind of fields such as active learning\cite{gal2017deep}, reinforcement learning\cite{blundell2015weight}\cite{osband2016deep}\cite{gal2016improving} %\maxcom{reinforcement learning does not really demand a human interaction...I added a part to the sentence before, so that also reinforcement learning is correct. is it fine like this?}
, automatic labeling, industrial components inspection 
%\maxcom{why does this require human interaction? isn't this just a use case for learning?} 
and so on. The idea behind that is, with reliable model uncertainty estimation we know which data sample or prediction the model is unfamiliar with 
%\maxcom{this sentence relies more to the out-of-distribution no?}
. In active or reinforcement learning, those unfamiliar data samples can be used to improve the corresponding model more efficiently. In tasks involving human robot interaction, the uncertainty estimation indicates when the model requires help from human experts. For instance, in industrial component inspection task, firstly a model which can perform quite well on some evaluation datasets is trained. With the assumption that all predictions made by the model are correct, the system is deployed to the real application.
% and deployed them in the real application. At this moment, we assume that all predictions made by the model are true all the time. 
However, there is no guarantee that this model represents the real distribution of data perfectly and is robust against a slight or even large domain transfer. If the distributions of the real world data vary because of unexpected factors e.g. equipment aging or changes of weather condition, the model could fail silently and lead to unexpected accidents. With reliable model uncertainty estimation, this kind of weakness can be mitigated or even eliminated. 

On the other hand, aleatoric uncertainty mainly assists in \textbf{improving model performance} by quantifying and considering this kind of information by representing noisy level of predictions and making good use of data points with less noise. By taking this information into account, the model can be trained more optimally and yield a better performance, some examples of image data and Lidar data can be referred to \cite{kendall2016modelling}\cite{feng2018towards}. Instead of representing noise, data uncertainty also includes uncertainty from a semantic view point, which we can be also incorporated into training and yield more robust models. This kind of uncertainty is not quantified explicitly but can be incorporated conceptually with the help of specifically designed models and high level statistic cues or features to \textbf{disambiguate predictions} directly. One example of this is the modeling contextual information among pixels by a so-called Conditional Random Field 
%\todo{you do not want to use abbreviations in your work?! check the glossary package ;)} 
in the semantic segmentation task\cite{krahenbuhl2011efficient}\cite{sutton2012introduction}\cite{lin2016efficient}. The idea behind that is simple and conditional random fields provide a principle way to achieve this idea. Since uncertainty caused by the appearance ambiguity represents strong correlation between different categories, a conditional random field(CRF) can model the correlation or dependency between random variables by taking into account high order potentials. In the task of semantic segmentation, due to the reason of tractable computation, at most second order potentials are taken into account and can yield better performance compared results with just first order potentials. 
%\maxcom{this part with the crf and semantic segmentation is a bit confusing, if one does not know how it works. Perhaps you go over the few sentences again...}
\section{How can we obtain and handle uncertainty in deep learning?}
As already known, deep learning is a powerful black box model while it has achieved tremendous success in different tasks. Therefore to obtain and handle uncertainty can help to get a better understanding about this black box model and make deep learning based applications more robust and safer. 

Regarding model uncertainty or quality measure that acts similar to model uncertainty, there are many different ways to obtain this kind of quality measure of prediction. On the one hand, due to the intractability of the real model uncertainty in deep neural networks, there are some \textbf{ad-hoc approaches} that try to obtain such a quality measure on specific tasks.
% which are solved usually with model uncertainty such as misclassified and out-of-distribution(OOD)\maxcom{you already used OOD before} detection task. 
\cite{liang2017enhancing} combine temperature scaling and input preprocessing which is called adversarial training together and yields a simple but effective OOD sample detector, but this approach requires an additional OOD dataset for training which is hard to collect in reality. \cite{devries2018learning} modify the loss function to train a classifier against OOD data, however this approach is hard to generalize in case of slight domain transfers. \cite{lee2017training} treat a uniform distribution as ground truth distribution of the OOD data and show that OOD data can be generated via a so-called Generative Adversarial Network(GAN). This approach seems practical, but training a GAN effectively does not always work for images because of the high dimensionality of the data distribution. 
%\maxcom{all three works are written by only one person?! otherwise you should leave the S at the end of the verbs}

On the other hand, there are some more theoretically sound approaches which employ \textbf{Bayesian neural networks}\cite{mackay1992practical}\cite{neal2012bayesian}, \textbf{bootstraping}\cite{osband2016deep}, \textbf{ensemble} methods\cite{lakshminarayanan2017simple} or even ensemble of Bayesian neural networks\cite{smith2018understanding}. Although a Bayesian neural network provides a principal way to obtain model uncertainty by putting randomness on model parameters, it is difficult to infer the exact posterior distribution over model parameters of a deep neural network because of its complex architecture and large scale dataset. Therefore an approximate inference plays an important role in addressing this issue. The golden standard for the approximate inference is Hamiltonian Monte Carlo\cite{neal2012bayesian}, which requires dealing with whole batch training data and storing the samples of posterior distribution. While the former issue is addressed partially by stochastic gradient Langevin Dynamics\cite{welling2011bayesian}, the latter one attracts many research interests like \cite{balan2015bayesian}\cite{wang2018adversarial}. However, since it is a Markov Chain Monte Carlo (MCMC) method, to assess the convergence is still non-trivial.

There are some alternatives to MCMC, one popular one is variational inference(VI)\cite{hinton1993keeping}. In VI, this inference problem is cast into optimization problem by minimizing the Kullbach-Leibler divergence between the approximate posterior distribution and real posterior distribution. Therefore approximate distribution family needs to be chosen and thus has a large impact on the result, which may impede the performance if the chosen distribution family is not flexible enough for the problem. \cite{graves2011practical} firstly employed fully factorized Gaussian with a biased gradient estimator on a practical problem. Later \cite{blundell2015weight} improved the result with the same approximate distribution but different estimator with help of "reparameterization trick"\cite{kingma2013auto}. \citealp{hernandez2015probabilistic} also used the same approximate distribution but with expectation propagation\cite{minka2001expectation} instead of VI.
Since the fully factorization assumption may impose an restriction on the flexibility of the approximate distribution between the correlations between weights could not be learned and expressed. By taking this into account, there are many attempts trying to use more expressive approximate distribution such as Bernoulli (treated as mixture of two Gaussian with small variance)\cite{gal2016dropout} and Gaussian \cite{kingma2015variational} which capture variances of rows in weight matrix. Another more expressive one is matrix variate Gaussian distribution\cite{louizos2016structured}\cite{sun2017learning}\cite{zhang2017noisy}, which capture both rows and columns correlation in weight matrix. Additionally inspired by the idea of normalizing flow in latent variable models \cite{louizos2017multiplicative}applied normalizing flows to auxiliary latent variables to produce more flexible approximate posteriors. 


Another alternative is the Laplace approximation\cite{mackay1992practical}, which puts a Gaussian centered at MAP estimate of the model parameters with a covariance determined by the inverse of the Hessian matrix of the log posterior distribution. The main issue of this approach for large neural networks is the large size of the Hessian matrix which is quadratic of the number of parameters and inversion operation on it whose complexity is cubic over the number of elements of Hessian matrix. Recently \cite{ritter2018scalable} adress this problem by approximating the Hessian or the Fisher matrix of log-likelihood with kronecked-factorization which reduces the storage size and complexity of inversion operation. The resulting posterior can also be treated as a matrix variate Gaussian.  

However, most of those ideas are verified with a relatively simple architecture and on toy or small-scale datasets and need to be modified a lot in order to serve the existing advanced architectures, which is undesirable in many practical applications. Therefore the so-called dropout inference\cite{gal2016dropout} looks promising considering this restriction and thus is adopted to obtain model uncertainty in this work. On the other hand, scalable Laplace approximation \cite{ritter2018scalable} has the potential to resolve this issue, because it requires only one MAP point estimate of model parameters, so there is no need to modify the training phase which could save a lot of computation effort and unnecessary modifications for many existing architectures.

When it comes to aleatoric uncertainty, as noise of data itself, it can be modeled by adding another head on top of the network \cite{kendall2017uncertainties}. However, the uncertainty caused by appearance ambiguity is not trivial to include in a deep neural network training. Therefore another model, \textbf{CRF}
%\maxcom{you already introduced CRF before}
\cite{lafferty2001conditional}, is adopted based on the predictions of a discriminative neural network classifier. As mentioned in the previous section, the choice of features of second order potentials is a key factor in this problem. In this work, addressing the scene object recognition task, the correlation between objects in specific scene are of importance. The correlation can be represented by a co-occurrence matrix \cite{ladicky2010graph}\cite{rasiwasia2009holistic}\cite{galleguillos2008object}\cite{rabinovich2007objects}.

\section{Contributions and Structure}
This work can be divided into two main parts, both focusing on uncertainty-based improvement for a deep learning based classifier on object recognition task. 

The first part is to employ dropout inference to obtain reliable and calibrated uncertainty from classifier with ResNet50\cite{he2016deep} as backbone. We evaluate the prediction and calibration performance of this probabilistic classifier and then select predictions with low uncertainty/high confidence based on uncertainty estimation as automatically labeled data which is used for training a more accurate classifier while the performance of original one declines a lot in case of slight domain transfer which is caused by possibly by changes of light condition, changes of appearance and even change of recording equipment. This is proof-of-concept idea which tries to prove that improved uncertainty estimation can assist in automatic labeling and reducing manual effort in fine-tuning a more accurate classifier in case of slight domain transfer during deployment of robot. In this part, we consider advanced version of dropout inference and its variants for improving the results both in terms of accuracy and calibration. 

The second part is to handle the uncertainty caused by appearance ambiguity. To note that again, this kind of uncertainty is not modeled directly, but the concept is taken into account in choosing model and features. To achieve this goal, we choose conditional random field to model the contextual relationship between objects with similar appearance but different contexts. As key factor of the model, we employ binary co-occurrence matrix of categories as pairwise features.

The main contributions of this work are:
\begin{itemize}
	\item to show that dropout inference and Laplace approximation can obtain better uncertainty information on a multi-class classification problem with 51 classes, which can be further improved by learning dropout rates during training and their ensemble.
	\item to show that manual effort in data collection in a slight domain transfer learning task can be reduced by making use of reliable uncertainty estimation.
	\item to show that uncertainty caused by appearance ambiguity can be resolved by incorporating contextual information and better uncertainty estimation via condition random field.
\end{itemize}

The structure of this work is as follows. In order to give the reader a clear theoretical background of models employed in this work, the basic theory and ideas of Bayesian neural network as well as dropout inference and Laplace approximation are briefly reviewed in chapter 2, where detailed adaptations and modifications of original model are described. In chapter 3, similar to previous chapter, the theory of a conditional random field is reviewed firstly and the adaptations are described. In chapter 4, different experiments and results about the evaluation of different variants of dropout inference are given and discussed firstly. In the following, experiments and results on scene recognition task resolved by conditional random field with co-occurrence statics as second order features are also showed and discussed. Then conclusion and possible future work are presented in chapter 5.

%\maxcom{I like the introduction and the related work :) it something new and not as the other works!!! A little bit more robotics could have been in there but that is fine :) it is nicely written just some minor changes (check my comments). Please check the citation type if you want more citations separately or together. and check the abbreviations (e.g. Conditional Random Field -> CRF, Deep Neural Network -> DNN and so on... to check when the first use of the term is used you could use the package glossary which checks this for you :) but cool! looks already professional :) ah and another thing is, I would not write in the first person singular (e.g. I show....) or plural (we show....). so I changed this here already. what do you think?}
